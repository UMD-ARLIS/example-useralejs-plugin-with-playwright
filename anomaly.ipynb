{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import json\n",
    "\n",
    "fs = s3fs.S3FileSystem(profile='default')\n",
    "\n",
    "log_lst = []\n",
    "for fname in fs.ls(\"msk-tap-raw-logs-sink/topics/raw/partition=0\"):\n",
    "    with fs.open(fname, 'rb') as f:\n",
    "        for line in f:\n",
    "            log_lst.append(json.loads(line))\n",
    "\n",
    "# with fs.open('s3://odni-model-weights/weights/model_5_dict.pth', 'rb') as f:\n",
    "#     # Test reading the file or just print a message that it's accessible\n",
    "#     print(\"Successfully accessed the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# logs:  9400\n"
     ]
    }
   ],
   "source": [
    "print(\"# logs: \", len(log_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the clicks, specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['logType', 'userAction', 'scrnRes', 'microTime', 'pageTitle', 'sessionID', 'type', 'clientTime', 'userId', 'target', 'path', 'toolVersion', 'browser', 'useraleVersion', 'pageUrl', 'location', 'details', 'pageReferrer', 'toolName'])\n",
      "dict_keys(['ctrl', 'meta', 'shift', 'alt', 'clicks'])\n",
      "click\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "print((log_lst[i].keys()))\n",
    "print((log_lst[i][\"details\"].keys()))\n",
    "print((log_lst[i][\"type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_lst = []\n",
    "for log in log_lst:\n",
    "    if log[\"type\"] == \"click\":\n",
    "        clicks_lst.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# click logs:  97\n"
     ]
    }
   ],
   "source": [
    "print(\"# click logs: \", len(clicks_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Anomaly Dector using River"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiration: https://medium.com/spikelab/anomalies-detection-using-river-398544d3536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Parameters like the number of trees (n_trees), the height of trees (height), and window size (window_size) significantly impact the model's sensitivity. Inappropriate values for these parameters might lead to poor anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the model works:\n",
    "*   Constructs \"half-spaced trees\" structure by randomly spliting feature spaces (vectors) into \"half-spaces\" according to their range of values\n",
    "*   As data-points are passed in from the root, at each node, based on the value of the selected feature and the split criterion at that node, the data point moves to the left or right child node\n",
    "*   The data points ultimately trickle down to a leaf node where counts of the nodes that reach each leaf are recorded\n",
    "*   If a data-point reaches an infrequent node, it will be considered an anomaly in proportion to the extent to which its ultimate leaf is infrequent\n",
    "\n",
    "In sum, it goes by **Denisty Estimation** - The idea that regions of the feature space that have lower density (i.e., fewer data points have reached the corresponding leaf nodes) are more likely to represent anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose\n",
    "from river import feature_extraction as fx\n",
    "from river import anomaly\n",
    "from river import preprocessing\n",
    "from river import stats\n",
    "from river import drift\n",
    "import numbers\n",
    "\n",
    "# Define a function to build the model\n",
    "def build_model(n_trees=25, height=10, window_size=250):\n",
    "    # Define the feature extraction pipeline\n",
    "    features_pipeline = compose.TransformerUnion(\n",
    "        compose.Select('logType', 'userAction', 'scrnRes', 'pageTitle', 'sessionID', 'type', \n",
    "                       'clientTime', 'userId', 'target', 'path', 'toolVersion', 'browser', \n",
    "                       'useraleVersion', 'pageUrl', 'location', 'details', 'pageReferrer', \n",
    "                       'toolName'),\n",
    "        \n",
    "        # Using Mean as a replacement for RollingMean\n",
    "        fx.Agg(on='microTime', by='sessionID', how=stats.Mean())\n",
    "        # Add more features and aggregations as needed\n",
    "    )\n",
    "\n",
    "    # Categorical features processing\n",
    "    categorical_features = compose.Pipeline(\n",
    "        compose.SelectType(str),\n",
    "        preprocessing.OneHotEncoder()\n",
    "    )\n",
    "\n",
    "    # Numerical features processing\n",
    "    numerical_features = compose.Pipeline(\n",
    "        compose.SelectType(numbers.Number),\n",
    "        preprocessing.MinMaxScaler()\n",
    "    )\n",
    "\n",
    "    # Combining everything into a single pipeline\n",
    "    model = compose.Pipeline(\n",
    "        features_pipeline,\n",
    "        numerical_features + categorical_features,\n",
    "        anomaly.HalfSpaceTrees(n_trees=n_trees, height=height, window_size=window_size)\n",
    "    )\n",
    "\n",
    "    # Add a drift detector to the model (e.g., ADWIN)\n",
    "    adwin_drift_detector = drift.ADWIN()\n",
    "    return model, adwin_drift_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly score for log 0: 0\n",
      "Anomaly score for log 1: 0\n",
      "Anomaly score for log 2: 0\n",
      "Anomaly score for log 3: 0\n",
      "Anomaly score for log 4: 0\n",
      "Anomaly score for log 5: 0\n",
      "Anomaly score for log 6: 0\n",
      "Anomaly score for log 7: 0\n",
      "Anomaly score for log 8: 0\n",
      "Anomaly score for log 9: 0\n",
      "Anomaly score for log 10: 0\n",
      "Anomaly score for log 11: 0\n",
      "Anomaly score for log 12: 0\n",
      "Anomaly score for log 13: 0\n",
      "Anomaly score for log 14: 0\n",
      "Anomaly score for log 15: 0\n",
      "Anomaly score for log 16: 0\n",
      "Anomaly score for log 17: 0\n",
      "Anomaly score for log 18: 0\n",
      "Anomaly score for log 19: 0\n",
      "Anomaly score for log 20: 0\n",
      "Anomaly score for log 21: 0\n",
      "Anomaly score for log 22: 0\n",
      "Anomaly score for log 23: 0\n",
      "Anomaly score for log 24: 0\n",
      "Anomaly score for log 25: 0\n",
      "Anomaly score for log 26: 0\n",
      "Anomaly score for log 27: 0\n",
      "Anomaly score for log 28: 0\n",
      "Anomaly score for log 29: 0\n",
      "Anomaly score for log 30: 0\n",
      "Anomaly score for log 31: 0\n",
      "Anomaly score for log 32: 0\n",
      "Anomaly score for log 33: 0\n",
      "Anomaly score for log 34: 0\n",
      "Anomaly score for log 35: 0\n",
      "Anomaly score for log 36: 0\n",
      "Anomaly score for log 37: 0\n",
      "Anomaly score for log 38: 0\n",
      "Anomaly score for log 39: 0\n",
      "Anomaly score for log 40: 0\n",
      "Anomaly score for log 41: 0\n",
      "Anomaly score for log 42: 0\n",
      "Anomaly score for log 43: 0\n",
      "Anomaly score for log 44: 0\n",
      "Anomaly score for log 45: 0\n",
      "Anomaly score for log 46: 0\n",
      "Anomaly score for log 47: 0\n",
      "Anomaly score for log 48: 0\n",
      "Anomaly score for log 49: 0\n",
      "Anomaly score for log 50: 0\n",
      "Anomaly score for log 51: 0\n",
      "Anomaly score for log 52: 0\n",
      "Anomaly score for log 53: 0\n",
      "Anomaly score for log 54: 0\n",
      "Anomaly score for log 55: 0\n",
      "Anomaly score for log 56: 0\n",
      "Anomaly score for log 57: 0\n",
      "Anomaly score for log 58: 0\n",
      "Anomaly score for log 59: 0\n",
      "Anomaly score for log 60: 0\n",
      "Anomaly score for log 61: 0\n",
      "Anomaly score for log 62: 0\n",
      "Anomaly score for log 63: 0\n",
      "Anomaly score for log 64: 0\n",
      "Anomaly score for log 65: 0\n",
      "Anomaly score for log 66: 0\n",
      "Anomaly score for log 67: 0\n",
      "Anomaly score for log 68: 0\n",
      "Anomaly score for log 69: 0\n",
      "Anomaly score for log 70: 0\n",
      "Anomaly score for log 71: 0\n",
      "Anomaly score for log 72: 0\n",
      "Anomaly score for log 73: 0\n",
      "Anomaly score for log 74: 0\n",
      "Anomaly score for log 75: 0\n",
      "Anomaly score for log 76: 0\n",
      "Anomaly score for log 77: 0\n",
      "Anomaly score for log 78: 0\n",
      "Anomaly score for log 79: 0\n",
      "Anomaly score for log 80: 0\n",
      "Anomaly score for log 81: 0\n",
      "Anomaly score for log 82: 0\n",
      "Anomaly score for log 83: 0\n",
      "Anomaly score for log 84: 0\n",
      "Anomaly score for log 85: 0\n",
      "Anomaly score for log 86: 0\n",
      "Anomaly score for log 87: 0\n",
      "Anomaly score for log 88: 0\n",
      "Anomaly score for log 89: 0\n",
      "Anomaly score for log 90: 0\n",
      "Anomaly score for log 91: 0\n",
      "Anomaly score for log 92: 0\n",
      "Anomaly score for log 93: 0\n",
      "Anomaly score for log 94: 0\n",
      "Anomaly score for log 95: 0\n",
      "Anomaly score for log 96: 0\n"
     ]
    }
   ],
   "source": [
    "# Build the model and drift detector\n",
    "model, drift_detector = build_model()\n",
    "\n",
    "# Example of training the model on your 'clicks_lst' data\n",
    "for log in clicks_lst:\n",
    "    model.learn_one(log)  # No reassignment needed\n",
    "\n",
    "# Example of using the model to detect anomalies and monitor for concept drift\n",
    "for i, log in enumerate(clicks_lst):\n",
    "    # Get anomaly score\n",
    "    score = model.score_one(log)\n",
    "    print(f\"Anomaly score for log {i}: {score}\")\n",
    "\n",
    "    # Update the drift detector with the anomaly score\n",
    "    drift_detector.update(score)\n",
    "    if drift_detector.drift_detected:\n",
    "        # React to the drift, e.g., retrain the model, log the drift, etc.\n",
    "        print(\"Change detected at index\", i)\n",
    "        # Reset the drift detector\n",
    "        drift_detector.reset()\n",
    "\n",
    "    # Define a threshold to decide if a log is anomalous (if needed)\n",
    "    # if score > some_threshold:\n",
    "    #     print(\"Anomaly detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Anomalous Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "*   Build webcrawler that does one or many workflows in a reasonable fashion - try playwright library to asynchronously open a browser and does any number of steps \n",
    "*   Enter user information, login, navigate to repo, review files, and log out, etc. - needs wait time\n",
    "*   Make another workflow that's weird - login, click on weird places, click repo, clicking a button a bunch of times, no waittime\n",
    "*   Could log it to Kafka or do it locally\n",
    "*   First, work on developing the workflow bots, Evan's gonna send the docs and example scripts and I'll get one working before we figure out uploading that to and extracting from buckets\n",
    "*   Make a new repository for this, install with pip, \n",
    "*   Not sure if it'll work with userale, since it uses a seperate browser. Question is if it uses the plugin as well\n",
    "*   Ensure the plugin is installed on chromium - Evan will investigate if it does/doesnt\n",
    "*   Rageclick, anomalous click me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All developments on this front exist in the test-userale-plugin.py file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 (odni)",
   "language": "python",
   "name": "odni"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
